---
# Documentation: https://wowchemy.com/docs/managing-content/

title: "Deep Audio-Visual Model for Speech Recognition"
summary: "Research on infusing attention mechanism for combined Deep Audio-Visual Models"
authors: ["Devesh Walawalkar","Yihui Ye","Rohit Pillai"]
tags: ["Speech Recognition","Computer Vision","Research"]
categories: []
date: 2018-12-15

# Optional external URL for project (replaces project detail page).
external_link: ""

# Featured image
# To use, add an image named `featured.jpg/png` to your page's folder.
# Focal points: Smart, Center, TopLeft, Top, TopRight, Left, Right, BottomLeft, Bottom, BottomRight.
image:
  caption: "Combined audio-visual model for speech recognition"
  focal_point: "Bottom"
  preview_only: false

# Custom links (optional).
#   Uncomment and edit lines below to show custom links.
# links:
# - name: Follow
#   url: https://twitter.com
#   icon_pack: fab
#   icon: twitter

url_code: ""
url_pdf: "https://arxiv.org/pdf/1812.09336.pdf"
url_slides: ""
url_video: ""

# Slides (optional).
#   Associate this project with Markdown slides.
#   Simply enter your slide deck's filename without extension.
#   E.g. `slides = "example-slides"` references `content/slides/example-slides.md`.
#   Otherwise, set `slides = ""`.
slides: ""
---

<div style="text-align: justify"> In this project, work was conducted on speech recognition, specifically predicting individual words based on both the video frames and audio. Empowered by convolutional neural networks, the recent speech recognition and lip reading models are comparable to human level performance. We reimplemented and made derivations of the state-of-the-art model.  Then, we conducted experiments including the effectiveness of attention mechanism, more accurate residual network as the backbone with pre-trained weights and the sensitivity of our model with respect to audio input with/without noise. </div>